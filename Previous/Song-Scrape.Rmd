Scraping Billboard Year-End Hot 100 Songs and Lyrics from the web
===============================================
```{r load_packages, echo=FALSE, message=FALSE, warning=FALSE}
library("XML")
library("stringr")
library("knitr")
opts_chunk$set(eval=FALSE, warning=FALSE, message=FALSE)
```

## Scrape songs
For each year in 1964-2014, create a URL for the Wikipedia Billboard Year-End Hot 100 songs entry. Collect song name, rank and artist name data from eaach of the 100 table rows. Bind them into a neat dataframe (`allthesongs`) with four variables, Rank, Song name, Artist Name and Year, and 5100 rows (51 years of Top 100 songs).

```{r scrape_songs, cache=TRUE}
allthesongs <- data.frame() 

for (i in 1964:2014) { 
     
     # create the URL for each year
     URL <- paste("http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_",i,sep="")
     
     # parse the HTML
     results <- htmlTreeParse(URL, useInternal=TRUE)
     
     # pull out text from each table row
     billboard_text <- xpathSApply(results, "//table[@class='wikitable sortable']//tr",xmlValue)
     
     # split into three pieces (rank, song name and year)
     split_billboard_text <- str_split_fixed(billboard_text,"\n",3) 
     
     # put artist names into a data frame along with the year it is from
     billboard <- as.data.frame(cbind(split_billboard_text[2:101, ], rep(i,100)), stringsAsFactors=FALSE)
     
     # row bind this year's data to all the data
     allthesongs <- rbind(allthesongs, billboard) 
     
}

colnames(allthesongs) <- c("Rank", "Song", "Artist", "Year")

```

## Clean up the songs and artists  

Remove any non-alphanumeric characters and handle a few exceptions directly (ampersands, accents, dashes and a few particularly difficult artist names). This will help standardize the entries for use in created URLs when scraping lyrics.


```{r clean_songs}
allthesongs$Song <- gsub("[^[:alnum:] ]", "", allthesongs$Song)
allthesongs$Artist <- gsub("& ", "", allthesongs$Artist) 
# fix accents
    allthesongs$Artist <- gsub("è|Ã|é", "e", allthesongs$Artist) 
    allthesongs$Artist <- gsub("ý", "y", allthesongs$Artist) 
    allthesongs$Artist <- gsub("ó|ō|ö", "o", allthesongs$Artist)
     allthesongs$Artist <- gsub("ü", "u", allthesongs$Artist)
    allthesongs$Artist <- gsub("-", " ", allthesongs$Artist)
allthesongs$Artist <- gsub("[^[:alnum:] ]", "", allthesongs$Artist)
# fix a few troublesome artists
     allthesongs$Artist <- gsub("Olivia Newton John", "Olivia NewtonJohn", allthesongs$Artist)
     allthesongs$Artist <- gsub("Ne Yo", "NeYo", allthesongs$Artist)  
     allthesongs$Artist <- gsub("Jay Z", "JayZ", allthesongs$Artist)
     allthesongs$Artist <- gsub("N Sync", "Nsync", allthesongs$Artist)
     allthesongs$Artist <- gsub("Daryl Hall John Oates", "Hall Oates", allthesongs$Artist)
```

## Scrape Lyrics  
For each song, create four viable URLs (metrolyrics.com, metrolyics.com without a "The" in the artist name [e.g. Beatles instead of The Beatles],  songlyrics.com and finally songlyrics.com without a "The" in the artist name). Use TryCatch to try to scrape lyrics from the first URL; if unsuccessful, try the second, then the third, and so on. Once lyrics have been found, break the loop and record the successful source number under the variable Source.

About 80% of the lyrics are available from [metrolyics.com](http://www.metrolyrics.com/). Of the remaining 20%, about 14% were found and scraped from 
[songlyrics.com](http://www.songlyrics.com/); 5.6% (288/5100) were unavailable.

Result: dataframe `allthesongs` with 5100 rows and 7 columns: Rank, Song name, Artist, Year, Lyrics and Source.

```{r scrape_lyrics, cache=TRUE, eval=FALSE}
allthesongs$Lyrics <- ""
allthesongs$Source <- ""

for (s in 1:length(allthesongs$Song))  {
     lyrics <- "Not set yet."
     results <- 12 # arbitrary number
     
     # clean up the artist field to fit in the URL
          artist <- strsplit(allthesongs$Artist[s], " featuring | feat | feat. | with | duet | and ")
          artist <- unlist(artist)[[1]]
          artist2 <- gsub("The ", "", artist)
               
     # make URLs
        metroURL <- paste("http://metrolyrics.com/",allthesongs$Song[s],"-lyrics-",artist2,".html",sep="")
        metronotheURL <- paste("http://metrolyrics.com/",allthesongs$Song[s],"-lyrics-",artist,".html",sep="")
        songURL <- paste("http://songlyrics.com/",artist2,"/",allthesongs$Song[s],"-lyrics",sep="")
        songnotheURL <- paste("http://songlyrics.com/",artist,"/",allthesongs$Song[s],"-lyrics",sep="")
          
     URLs <- c(metroURL, metronotheURL, songURL, songnotheURL)
     
     lyriclocs <- c("//div[@id='lyrics-body-text']", 
                    "//div[@id='lyrics-body-text']", 
                    "//p[@id='songLyricsDiv']", 
                    "//p[@id='songLyricsDiv']")
     
     for (b in 1:length(URLs)) {
        allthesongs$Lyrics[s] <- "Not set yet."
        
        results <- 12 # arbitrary number
        
        URL <- tolower(gsub(" ", "-", URLs[b]))
        
          tryCatch({ 
               results <- htmlTreeParse(URL, useInternal=TRUE, isURL=TRUE)
               lyrics <- xpathSApply(results, lyriclocs[b], xmlValue) },
          error = function(x) { 
              message(paste(s, "failed")) },
          finally={ 
               if (!is.numeric(results)) { 
                    if (length(lyrics)!=0) { 
                      allthesongs$Lyrics[s] <- lyrics[[1]]
                      message(paste(s, "success"))
                      allthesongs$Source[s] <- b
                      break
                    }
               } 
          }) # end tryCatch
     } # end URL for
} # end for
```

## Clean up the lyrics

```{r clean_lyrics}
allthesongs$Lyrics <- gsub("\\\n|\\\t"," ",allthesongs$Lyrics)
allthesongs$Lyrics <- tolower(gsub("[^[:alnum:] ]", "", allthesongs$Lyrics))
```

## Write the file  

```{r write_file}
setwd("/Users/kaylinwalker/R/kw_musiclyrics")
write.csv(allthesongs, "final_all_songs.csv", row.names=FALSE)
```

